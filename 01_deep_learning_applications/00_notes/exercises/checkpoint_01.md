1. "Dropout is a technique that randomly deactivates neurons during training to reduce overfitting, while during inference all neurons are used with their weights scaled appropriately."

2. "Which activation function is commonly used in neural network hidden layers due to its computational efficiency and ability to mitigate the vanishing gradient problem?"
Answer: ReLU

3. "What are the main disadvantages associated with using Sigmoid and Tanh activation functions in deep neural networks?"
Answer: They can lead to vanishing gradient problems, making learning difficult in deep layers.

4. "For regression problems, what are the two most common loss functions?"
Answer: Mean Squared Error (MSE) and Mean Absolute Error (MAE)

5. "Which of the following problems can be introduced by activation functions?"
Answer: Vanishing or exploding gradients

6. "Which of the following statements best explains why non-linearity introduced by activation functions is crucial for neural network performance?"
Answer: It enables the network to learn and model complex, non-linear relationships in the data.

7. "Which of the following statements best describes the purpose of using a Learning Rate Scheduler during deep learning model training?"
Answer: To modify the learning rate over time, usually decreasing it to allow more precise convergence.

8. "What does the optimization process do after backpropagation calculates the loss function gradient?"
Answer: Iteratively adjusts the network weights in the opposite direction of the gradient.

9. "Which of the following is not an optimizer in the context of neural network training?"
Answer: Backtracking Line Search

10. "Techniques like Dropout, Early Stopping, L1, and L2 regularization can be used in both deep learning and traditional machine learning to prevent overfitting."

11. "What is the main function of the backpropagation algorithm in Deep Learning neural networks?"
Answer: Calculate the gradient of the loss function with respect to each network weight.

12. "Why can decreasing the learning rate over time be beneficial during deep learning model training?"
Answer: It allows for finer optimization adjustments near the optimum and helps avoid overshooting, potentially leading to better final results.

13. "Which activation function is known for generating outputs between 0 and infinity and is currently the most used in deep neural networks due to its computational efficiency?"
Answer: ReLU

14. "During the backpropagation process, calculating the gradient of the loss function with respect to each weight in the network is performed after calculating the error."

15. A proper answer would need to specify something that is actually not a Learning Rate Scheduler.

16. "Regularization, an essential technique for preventing overfitting, can be implemented in various ways, including adding regularization terms to the loss function, using Dropout, and Early Stopping."
Answer: True

17. "Which of the following statements best describes the role of the loss function in machine learning and Deep Learning?"
Answer: The loss function quantifies how well a model's predictions align with the observed true values, aiming for its minimization during training.

18. "In the context of the 'scaled dot-product' attention mechanism within the Transformer architecture, how are the Q, K, and V components derived in the encoder?"
Answer: Q, K, and V are all derived from the same input in the encoder through different linear transformations.