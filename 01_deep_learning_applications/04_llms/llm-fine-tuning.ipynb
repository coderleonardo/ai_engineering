{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate peft bitsandbytes transformers trl datasets torch\n!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:54:35.654682Z","iopub.execute_input":"2025-01-18T18:54:35.654942Z","iopub.status.idle":"2025-01-18T18:54:46.236197Z","shell.execute_reply.started":"2025-01-18T18:54:35.654911Z","shell.execute_reply":"2025-01-18T18:54:46.235154Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:54:46.237201Z","iopub.execute_input":"2025-01-18T18:54:46.237816Z","iopub.status.idle":"2025-01-18T18:54:46.242889Z","shell.execute_reply.started":"2025-01-18T18:54:46.237789Z","shell.execute_reply":"2025-01-18T18:54:46.242086Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import accelerate\nimport peft\nimport bitsandbytes\nimport transformers\nimport trl\nimport datasets\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers import TrainingArguments\nfrom peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:55:05.079912Z","iopub.execute_input":"2025-01-18T18:55:05.080566Z","iopub.status.idle":"2025-01-18T18:55:08.108983Z","shell.execute_reply.started":"2025-01-18T18:55:05.080531Z","shell.execute_reply":"2025-01-18T18:55:08.108361Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(torch.cuda.device_count(), torch.cuda.get_device_name(0), torch.cuda.get_device_properties(0).total_memory / 1e9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:55:08.109766Z","iopub.execute_input":"2025-01-18T18:55:08.109964Z","iopub.status.idle":"2025-01-18T18:55:08.115149Z","shell.execute_reply.started":"2025-01-18T18:55:08.109946Z","shell.execute_reply":"2025-01-18T18:55:08.114191Z"}},"outputs":[{"name":"stdout","text":"2 Tesla T4 15.828320256\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset = load_dataset(\"nlpie/Llama2-MedTuned-Instructions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:56:31.948557Z","iopub.execute_input":"2025-01-18T18:56:31.948851Z","iopub.status.idle":"2025-01-18T18:56:36.450825Z","shell.execute_reply.started":"2025-01-18T18:56:31.948830Z","shell.execute_reply":"2025-01-18T18:56:36.450177Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f789873a397d4753ae149366e341d210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-a8790d88efc2bc45.parquet:   0%|          | 0.00/91.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d1f72396584278935c8089eab835b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-b543c64b1786c03e.parquet:   0%|          | 0.00/6.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc777e5ffee4c15a80d51971e2c4b8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/200252 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a69c69bbe091471c9d163ba0205bd71c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/70066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b8f3f259734fdbbfc63ca96c3200ac"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:58:20.729096Z","iopub.execute_input":"2025-01-18T18:58:20.729464Z","iopub.status.idle":"2025-01-18T18:58:20.734375Z","shell.execute_reply.started":"2025-01-18T18:58:20.729432Z","shell.execute_reply":"2025-01-18T18:58:20.733547Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 200252\n    })\n    validation: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 70066\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Show the first 3 rows\nfor i in range(3):\n    data = dataset['train'][i]\n    print(f\"Data Point {i + 1}:\")\n    print(\"Instruction >>>\", data['instruction'])\n    print(\"Input       >>>\", data['input'])\n    print(\"Output      >>>\", data['output'])\n    print(\"\\n-----------------------------\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:58:27.968895Z","iopub.execute_input":"2025-01-18T18:58:27.969224Z","iopub.status.idle":"2025-01-18T18:58:27.977890Z","shell.execute_reply.started":"2025-01-18T18:58:27.969183Z","shell.execute_reply":"2025-01-18T18:58:27.977076Z"}},"outputs":[{"name":"stdout","text":"Data Point 1:\nInstruction >>> In your role as a medical professional, address the user's medical questions and concerns.\nInput       >>> My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.\nOutput      >>> Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\n\n-----------------------------\n\nData Point 2:\nInstruction >>> Your role as a doctor requires you to answer the medical questions taking into account the patient's description.\nAnalyze the question given its context. Give both long answer and yes/no decision.\nInput       >>> ###Question: Are fibrocytes involved in inflammation as well as fibrosis in the pathogenesis of Crohn 's disease?\n###Context: We previously showed that fibrocytes, a hematopoietic stem cell source of fibroblasts/myofibroblasts, infiltrated the colonic mucosa of a murine colitis model. We investigated whether fibrocytes were involved in the pathogenesis of Crohn's disease. Human surgical intestinal specimens were stained with anti-leukocyte-specific protein 1 and anti-collagen type-I (ColI) antibodies. Circulating fibrocytes in the human peripheral blood were quantified by fluorescence-activated cell sorting with anti-CD45 and anti-ColI antibodies. Cultured human fibrocytes were prepared by culturing peripheral CD14(+) monocytes. In the specimens of patients with Crohn's disease, the fibrocyte/total leukocyte percentage was significantly increased in inflammatory lesions (22.2 %, p < 0.01) compared with that in non-affected areas of the intestine (2.5 %). Interestingly, the percentage in fibrotic lesions was similar (2.2 %, p = 0.87) to that in non-affected areas. The percentages of circulating fibrocytes/total leukocytes were significantly higher in patients with Crohn's disease than in healthy controls. Both CXC-chemokine receptor 4(+) and intercellular adhesion molecule 1(+) fibrocyte numbers were significantly increased in Crohn's disease, suggesting that circulating fibrocytes have a higher ability to infiltrate injured sites and traffic leukocytes. In cultured fibrocytes, lipopolysaccharide treatment remarkably upregulated tumor necrosis factor (TNF)-α mRNA (17.0 ± 5.7-fold) and ColI mRNA expression (12.8 ± 5.7-fold), indicating that fibrocytes stimulated by bacterial components directly augmented inflammation as well as fibrosis.\nOutput      >>> Fibrocytes are recruited early in the inflammatory phase and likely differentiate into fibroblasts/myofibroblasts until the fibrosis phase. They may enhance inflammation by producing TNF-α and can directly augment fibrosis by producing ColI.\n\n###Answer: yes\n\n-----------------------------\n\nData Point 3:\nInstruction >>> Your identity is a doctor, kindly provide answers to the medical questions with consideration of the patient's description.\nAnalyze the question and answer with the best option.\nInput       >>> ###Question: Afterhyperpolarization due to\n###Options:\nA. Na efflux\nB. Na+ influx\nC. CI influx\nD. K+ efflux\n\nOutput      >>> ###Rationale: Slow return of the K+ channels to the closed state thus K+ efflux.(Ref: Textbook of physiology AK Jain 5th edition page no.36)\n\n###Answer: OPTION D IS CORRECT.\n\n-----------------------------\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# selecting some data to train the model fast\ndataset[\"train\"] = dataset[\"train\"].select(range(10000))\ndataset[\"test\"]  = dataset[\"validation\"].select(range(1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:59:26.007264Z","iopub.execute_input":"2025-01-18T18:59:26.007670Z","iopub.status.idle":"2025-01-18T18:59:26.016164Z","shell.execute_reply.started":"2025-01-18T18:59:26.007639Z","shell.execute_reply":"2025-01-18T18:59:26.015412Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:59:27.023274Z","iopub.execute_input":"2025-01-18T18:59:27.023590Z","iopub.status.idle":"2025-01-18T18:59:27.028270Z","shell.execute_reply.started":"2025-01-18T18:59:27.023564Z","shell.execute_reply":"2025-01-18T18:59:27.027469Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 10000\n    })\n    validation: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 70066\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# creating the prompt \ndef create_prompt(sample):\n    prompt = str(sample[\"instruction\"])\n    prompt += \" \" + str(sample[\"input\"])\n    \n    single_turn_prompt = f\"\"\"Instruction: {prompt}<|end_of_turn|> \\n\\nAI Assistant: {sample[\"output\"]}\"\"\"\n    return [single_turn_prompt]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:59:42.101313Z","iopub.execute_input":"2025-01-18T18:59:42.101641Z","iopub.status.idle":"2025-01-18T18:59:42.105682Z","shell.execute_reply.started":"2025-01-18T18:59:42.101612Z","shell.execute_reply":"2025-01-18T18:59:42.104696Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(create_prompt(dataset['train'][0])[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:59:46.389634Z","iopub.execute_input":"2025-01-18T18:59:46.389945Z","iopub.status.idle":"2025-01-18T18:59:46.394560Z","shell.execute_reply.started":"2025-01-18T18:59:46.389920Z","shell.execute_reply":"2025-01-18T18:59:46.393604Z"}},"outputs":[{"name":"stdout","text":"Instruction: In your role as a medical professional, address the user's medical questions and concerns. My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.<|end_of_turn|> \n\nAI Assistant: Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(create_prompt(dataset['train'][10])[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:59:47.676344Z","iopub.execute_input":"2025-01-18T18:59:47.676653Z","iopub.status.idle":"2025-01-18T18:59:47.681673Z","shell.execute_reply.started":"2025-01-18T18:59:47.676627Z","shell.execute_reply":"2025-01-18T18:59:47.680992Z"}},"outputs":[{"name":"stdout","text":"Instruction: In the clinical text, your objective is to identify relationships between medical problems, treatments, and tests. Medical problems are tagged as @problem$, medical tests as @test$, and treatments as @treatment$. Classify the relationship between two entities as one of the following:\nTreatment improves medical problem (TrIP)\nTreatment worsens medical problem (TrWP)\nTreatment causes medical problem (TrCP)\nTreatment is administered for medical problem (TrAP)\nTreatment is not administered because of medical problem (TrNAP)\nTest reveals medical problem (TeRP)\nTest conducted to investigate medical problem (TeCP)\nMedical problem indicates medical problem (PIP)\nNo Relations Digoxin 0.125 mg q.d. , @treatment$ 80 mg q.a.m. and 40 mg q.p.m. aspirin 1 q.d. , and @treatment$ three puffs b.i.d.<|end_of_turn|> \n\nAI Assistant: No Relations\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Quantization step\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_compute_dtype=\"float16\",\n                                bnb_4bit_use_double_quant=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:00:04.359211Z","iopub.execute_input":"2025-01-18T19:00:04.359580Z","iopub.status.idle":"2025-01-18T19:00:04.364886Z","shell.execute_reply.started":"2025-01-18T19:00:04.359551Z","shell.execute_reply":"2025-01-18T19:00:04.363980Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Loading the LLM\n# https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\n\nrepository_hf = \"berkeley-nest/Starling-LM-7B-alpha\"\n\n\n# Load the LLM applying quantization\nllm_model = AutoModelForCausalLM.from_pretrained(\n    repository_hf,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    use_cache=False\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:00:06.794121Z","iopub.execute_input":"2025-01-18T19:00:06.794445Z","iopub.status.idle":"2025-01-18T19:06:32.329391Z","shell.execute_reply.started":"2025-01-18T19:00:06.794417Z","shell.execute_reply":"2025-01-18T19:06:32.328524Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"246b4c9c8fd2404caebef222d972c7af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf298522bd394c129e30f6e026d2e857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64144e8b66b642d9a1ce2d09b9ba57e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9696fd943de34128bfec95d20c780da8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5de9e7594c14796b02066cff176aea3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3fbb85af5c34aaf9a1e945bbb5a3f2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89181e62c6da488fb9fa56c3c33253bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012aa506cda54b96b1bb3970adf58773"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Load the LLM tokenizer\ntokenizer = AutoTokenizer.from_pretrained(repository_hf)\n\n# Padding \ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:06:32.330459Z","iopub.execute_input":"2025-01-18T19:06:32.330689Z","iopub.status.idle":"2025-01-18T19:06:33.271333Z","shell.execute_reply.started":"2025-01-18T19:06:32.330668Z","shell.execute_reply":"2025-01-18T19:06:33.270584Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c319a34492fe486ca1f3bd0952f69fcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cdd5a9bb3254eca98bc3f793910948c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a780eea65e5d47dc814f90a9fceed294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5441401e9e44a50b740894703ecb01f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4251182383204ebbabd8498185979f5a"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"tokenizer.eos_token, tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:06:33.272472Z","iopub.execute_input":"2025-01-18T19:06:33.272747Z","iopub.status.idle":"2025-01-18T19:06:33.277752Z","shell.execute_reply.started":"2025-01-18T19:06:33.272714Z","shell.execute_reply":"2025-01-18T19:06:33.277071Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('<|end_of_turn|>', 32000)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"def generate_response_before_fine_tuning(prompt, model):\n\n    # Apply tokenizer\n    encoded_input = tokenizer(prompt,\n                              return_tensors=\"pt\", # pytorch\n                              add_special_tokens=True)\n\n    # Input to tensor\n    model_inputs = encoded_input.to(\"cuda\")\n\n    # Generate response\n    generated_ids = model.generate(**model_inputs,\n                                   max_new_tokens=1024,\n                                   do_sample=True,\n                                   pad_token_id=tokenizer.eos_token_id)\n\n    # Decoding the response\n    decoded_output = tokenizer.batch_decode(generated_ids)\n\n    return decoded_output[0].replace(prompt, \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:06:39.566794Z","iopub.execute_input":"2025-01-18T19:06:39.567124Z","iopub.status.idle":"2025-01-18T19:06:39.571779Z","shell.execute_reply.started":"2025-01-18T19:06:39.567093Z","shell.execute_reply":"2025-01-18T19:06:39.570994Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Example\nprompt = \"\"\"Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\nContradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. \"\"\"\nprompt += '''Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|>'''\nprompt += \"\\n\\nAI Assistant: \"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:06:39.852009Z","iopub.execute_input":"2025-01-18T19:06:39.852226Z","iopub.status.idle":"2025-01-18T19:06:39.856114Z","shell.execute_reply.started":"2025-01-18T19:06:39.852207Z","shell.execute_reply":"2025-01-18T19:06:39.855191Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(generate_response_before_fine_tuning(prompt, llm_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:06:40.108680Z","iopub.execute_input":"2025-01-18T19:06:40.108892Z","iopub.status.idle":"2025-01-18T19:06:58.597953Z","shell.execute_reply.started":"2025-01-18T19:06:40.108874Z","shell.execute_reply":"2025-01-18T19:06:58.596790Z"}},"outputs":[{"name":"stdout","text":"<s> Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\nContradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|> \n\nAI Assistant: \n\nAnswer: Entailment\n\nExplanation: Sentence 1 provides a clinical statement about a patient with hypotension that was confirmed by autonomic testing as orthostatic hypotension. Sentence 2 also states that a patient has orthostatic hypotension. The information provided in Sentence 1 logically entails the statement made in Sentence 2, as both discuss the same diagnosis of orthostatic hypotension. \n\nTherefore, the two sentences are in a relationship of entailment, where the information provided in the first sentence logically entails the information provided in the second sentence, but not necessarily vice versa. \n\nUnderstanding the relationship between the two sentences can be vital in interpreting clinical data, as contradictions can raise flags about misdiagnosis, while neutral sentences offer no predictive value on the patient's condition, and sentences with entailment can provide an element of consistency in the diagnosis. \n\nAll in all, categorizing sentences into their appropriate relationships is a crucial step in comprehending and analyzing clinical information.<|end_of_turn|>\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"torch.cuda.empty_cache()  # Clear the GPU memory cache\ntorch.cuda.init()  # Force GPU initialization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:07:06.846906Z","iopub.execute_input":"2025-01-18T19:07:06.847199Z","iopub.status.idle":"2025-01-18T19:07:06.867084Z","shell.execute_reply.started":"2025-01-18T19:07:06.847177Z","shell.execute_reply":"2025-01-18T19:07:06.866484Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Define LoRA parameters with PEFT\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:07:08.247176Z","iopub.execute_input":"2025-01-18T19:07:08.247567Z","iopub.status.idle":"2025-01-18T19:07:08.251478Z","shell.execute_reply.started":"2025-01-18T19:07:08.247533Z","shell.execute_reply":"2025-01-18T19:07:08.250688Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Prepare the model for the fine-tuning step\nllm_finetuned_model = prepare_model_for_kbit_training(llm_model)\n\n# Concatenate the base model with the LoRA parameters\nllm_finetuned_model = get_peft_model(llm_finetuned_model, peft_config)\n\n# llm_finetuned_model.to(device)  # Move the model to the GPU explicitly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:07:12.183742Z","iopub.execute_input":"2025-01-18T19:07:12.184030Z","iopub.status.idle":"2025-01-18T19:07:12.681015Z","shell.execute_reply.started":"2025-01-18T19:07:12.184006Z","shell.execute_reply":"2025-01-18T19:07:12.680358Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# # Model hyperparameters\n# accelerator = accelerate.Accelerator()\n# local_rank = accelerator.process_index\n# local_rank","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:07:24.988053Z","iopub.execute_input":"2025-01-18T19:07:24.988425Z","iopub.status.idle":"2025-01-18T19:07:24.992126Z","shell.execute_reply.started":"2025-01-18T19:07:24.988396Z","shell.execute_reply":"2025-01-18T19:07:24.991057Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:07:25.369856Z","iopub.execute_input":"2025-01-18T19:07:25.370077Z","iopub.status.idle":"2025-01-18T19:07:25.373825Z","shell.execute_reply.started":"2025-01-18T19:07:25.370058Z","shell.execute_reply":"2025-01-18T19:07:25.372931Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Checking LoRA\nprint(\"\\nChecking LoRA configurations:\")\nprint(f\"LoRA active modules: {llm_finetuned_model.peft_config['default'].target_modules}\")\nprint(f\"# trainable parameters: {sum(p.numel() for p in llm_finetuned_model.parameters() if p.requires_grad)}\")\n\ntrainable_params = []\nfor name, param in llm_finetuned_model.named_parameters():\n    if param.requires_grad:\n        trainable_params.append(name)\n        \nprint(f\"# trainable parameters list: {len(trainable_params)}\")\n# for param in trainable_params:\n#     print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:07:41.203738Z","iopub.execute_input":"2025-01-18T19:07:41.204015Z","iopub.status.idle":"2025-01-18T19:07:41.219850Z","shell.execute_reply.started":"2025-01-18T19:07:41.203993Z","shell.execute_reply":"2025-01-18T19:07:41.219149Z"}},"outputs":[{"name":"stdout","text":"\nChecking LoRA configurations:\nLoRA active modules: {'q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'}\n# trainable parameters: 20971520\n# trainable parameters list: 448\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./model_finetuned\",\n    \n    # Batch configuration and learning\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    \n    # Optimizer and scheduler\n    optim=\"adamw_torch\",  # paged_adamw_32bit\n    lr_scheduler_type=\"cosine\",\n    \n    # Precision and device\n    bf16=False,  # Unset bfloat16\n    fp16=True if torch.cuda.is_available() else False,  # Set fp16 if GPU is available\n    \n    # Saving and logging strategies\n    save_strategy=\"steps\",\n    save_steps=50,\n    save_total_limit=2,\n    logging_steps=1,\n    logging_strategy=\"steps\",\n    \n    # Epochs and evaluation\n    num_train_epochs=1,\n    max_steps=5, # 250,\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    \n    # Extra configurations\n    # remove_unused_columns=False,  # must have on SFTTrainer\n    disable_tqdm=False,\n    log_level=\"info\",\n    report_to=[],\n    \n    # GPU configurations\n    no_cuda=False if torch.cuda.is_available() else True,\n    \n    # Addition configurations\n    warmup_steps=50,\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n)\n\n# Supervised Fine-Tuning Trainer (SFTT) https://huggingface.co/docs/trl/sft_trainer\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=llm_finetuned_model,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    formatting_func=create_prompt,\n    args=training_arguments,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:08:08.917098Z","iopub.execute_input":"2025-01-18T19:08:08.917461Z","iopub.status.idle":"2025-01-18T19:08:19.535313Z","shell.execute_reply.started":"2025-01-18T19:08:08.917432Z","shell.execute_reply":"2025-01-18T19:08:19.534483Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a6e8f8fe7384eabb9bf65bbe5f48504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42dffbb6c3b41dc8c25c73b440aee97"}},"metadata":{}},{"name":"stderr","text":"You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(\"Pre-training verifications:\")\nprint(f\"# trainable paramaters : {sum(p.numel() for p in llm_finetuned_model.parameters() if p.requires_grad)}\")\nprint(f\"model device           : {next(llm_finetuned_model.parameters()).device}\")\nprint(f\"model data type        : {next(llm_finetuned_model.parameters()).dtype}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:08:26.335499Z","iopub.execute_input":"2025-01-18T19:08:26.335783Z","iopub.status.idle":"2025-01-18T19:08:26.347012Z","shell.execute_reply.started":"2025-01-18T19:08:26.335761Z","shell.execute_reply":"2025-01-18T19:08:26.346263Z"}},"outputs":[{"name":"stdout","text":"Pre-training verifications:\n# trainable paramaters : 20971520\nmodel device           : cuda:0\nmodel data type        : torch.float32\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# from transformers import logging\n\n# logging.set_verbosity_info()  # Set verbosity to show detailed information\n\n# print(\"Is GPU available?\", torch.cuda.is_available())\n# print(\"Using device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n\n# print(f\"Accelerate device: {accelerate.Accelerator().device}\")\n# print(f\"Current device index: {torch.cuda.current_device()}\")\n\n# torch.cuda.empty_cache()  # Clear the GPU memory cache\n# torch.cuda.init()  # Force GPU initialization\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:08:31.963755Z","iopub.execute_input":"2025-01-18T19:08:31.964036Z","iopub.status.idle":"2025-01-18T19:08:31.967464Z","shell.execute_reply.started":"2025-01-18T19:08:31.964014Z","shell.execute_reply":"2025-01-18T19:08:31.966475Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"%%time \ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:08:32.923810Z","iopub.execute_input":"2025-01-18T19:08:32.924076Z","iopub.status.idle":"2025-01-18T19:09:49.106845Z","shell.execute_reply.started":"2025-01-18T19:08:32.924055Z","shell.execute_reply":"2025-01-18T19:09:49.105621Z"}},"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 10\n  Num Epochs = 3\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 5\n  Number of trainable parameters = 20,971,520\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:59, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./model_finetuned/checkpoint-5\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--berkeley-nest--Starling-LM-7B-alpha/snapshots/1dddf3b95bc1391f6307299eb1c162c194bde9bd/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"openchat/openchat_3.5\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32002\n}\n\ntokenizer config file saved in ./model_finetuned/checkpoint-5/tokenizer_config.json\nSpecial tokens file saved in ./model_finetuned/checkpoint-5/special_tokens_map.json\nDeleting older checkpoint [model_finetuned/checkpoint-5] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 43.9 s, sys: 32 s, total: 1min 15s\nWall time: 1min 16s\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5, training_loss=1.1271696209907531, metrics={'train_runtime': 75.7842, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.066, 'total_flos': 788702309646336.0, 'train_loss': 1.1271696209907531, 'epoch': 1.8})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"import os\nimport shutil\n# from huggingface_hub import HfApi\n# from peft import PeftModel\n\ndef save_trained_model(trainer, output_dir=\"./model_finetuned\", zip_name=\"finetuned_model.zip\"):\n    # (model, tokenizer, output_dir=\"./model_output\", zip_name=\"trained_model.zip\")\n    \"\"\"\n    Save the model, the tokenizer and configurations in a zip file\n    \"\"\"\n\n    trainer.save_model(output_dir)\n    \n    # os.makedirs(output_dir, exist_ok=True)\n    \n    # # 1. Saving the LoRA model\n    # print(\"Saving LoRA adapters...\")\n    # model.save_pretrained(output_dir)\n    \n    # # 2. Saving the tokenizer\n    # print(\"Saving the tokenizer...\")\n    # tokenizer.save_pretrained(output_dir)\n    \n    # 3. Crie o arquivo zip\n    print(\"Zipping...\")\n    shutil.make_archive(zip_name.replace('.zip', ''), 'zip', output_dir)\n    \n    print(f\"Model saved at: {zip_name}\")\n    \n    # 4. File size\n    zip_size = os.path.getsize(zip_name) / (1024 * 1024)  # Tamanho em MB\n    print(f\"Zip size: {zip_size:.2f} MB\")\n    \n    # if os.path.exists('/kaggle/working'):\n    #     output_path = '/kaggle/working/' + zip_name\n    #     if os.path.exists(zip_name):\n    #         shutil.move(zip_name, output_path)\n    #         print(f\"File moved to: {output_path}\")\n\ndef load_model_with_memory_optimization(\n    base_model_name,\n    adapter_path,\n    use_4bit=True,\n    device_map=\"auto\"\n    ):\n    \"\"\"\n    Load the model with memory optimization\n    \"\"\"\n    # Clean CUDA memory\n    torch.cuda.empty_cache()\n    \n    # CUDA memory optimization config\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    \n    # 4-bits quantization\n    if use_4bit:\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n    else:\n        quantization_config = None\n    \n    try:\n        # Load model\n        print(\"Loading the base model...\")\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            quantization_config=quantization_config,\n            torch_dtype=torch.float16,\n            device_map=device_map,\n            offload_folder=\"offload\",  # Allow offload to CPU\n            offload_state_dict=True,   # Allow offload of state dict\n            low_cpu_mem_usage=True,    # Optimize the CPU memory usage\n        )\n        \n        # Load tokenizer\n        print(\"Load tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n        \n        # Load LoRA adapters\n        print(\"Applying LoRA adapters...\")\n        model = PeftModel.from_pretrained(\n            base_model,\n            adapter_path,\n            device_map=device_map,\n            is_trainable=False  # Define como False se for apenas para inferência\n        )\n        \n        print(\"Model loaded with success!\")\n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"Error loading the model: {str(e)}\")\n        if torch.cuda.is_available():\n            print(f\"Available GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n            print(f\"Used GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        raise e\n\n# save_trained_model(\n#     model=llm_finetuned_model,\n#     tokenizer=tokenizer,\n#     output_dir=\"./model_output\",\n#     zip_name=\"llm_finetuned_model.zip\"\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:10:55.533196Z","iopub.execute_input":"2025-01-18T19:10:55.533584Z","iopub.status.idle":"2025-01-18T19:10:55.541706Z","shell.execute_reply.started":"2025-01-18T19:10:55.533553Z","shell.execute_reply":"2025-01-18T19:10:55.540869Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# repository_hf = \"berkeley-nest/Starling-LM-7B-alpha\"\n\n# model, tokenizer = load_trained_model(\n#     base_model_name=repository_hf,  # ex: \"mistralai/Mistral-7B-v0.1\"\n#     adapter_path=\"./model_output\"\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:11:08.649732Z","iopub.execute_input":"2025-01-18T19:11:08.650014Z","iopub.status.idle":"2025-01-18T19:11:08.653529Z","shell.execute_reply.started":"2025-01-18T19:11:08.649992Z","shell.execute_reply":"2025-01-18T19:11:08.652609Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# import shutil\n\n# shutil.make_archive(\"trained_model.zip\".replace('.zip', ''), 'zip', \"./model_finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:11:09.533611Z","iopub.execute_input":"2025-01-18T19:11:09.533917Z","iopub.status.idle":"2025-01-18T19:11:09.537207Z","shell.execute_reply.started":"2025-01-18T19:11:09.533890Z","shell.execute_reply":"2025-01-18T19:11:09.536353Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"save_trained_model(trainer) # output_dir=\"./model_finetuned\", zip_name=\"finetuned_model.zip\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:11:11.474169Z","iopub.execute_input":"2025-01-18T19:11:11.474537Z","iopub.status.idle":"2025-01-18T19:11:41.918429Z","shell.execute_reply.started":"2025-01-18T19:11:11.474505Z","shell.execute_reply":"2025-01-18T19:11:41.917681Z"}},"outputs":[{"name":"stderr","text":"Saving model checkpoint to ./model_finetuned\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--berkeley-nest--Starling-LM-7B-alpha/snapshots/1dddf3b95bc1391f6307299eb1c162c194bde9bd/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"openchat/openchat_3.5\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32002\n}\n\ntokenizer config file saved in ./model_finetuned/tokenizer_config.json\nSpecial tokens file saved in ./model_finetuned/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"Zipping...\nModel saved at: finetuned_model.zip\nZip size: 516.98 MB\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"fine_tuned_model = llm_finetuned_model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:12:38.214890Z","iopub.execute_input":"2025-01-18T19:12:38.215180Z","iopub.status.idle":"2025-01-18T19:13:12.111944Z","shell.execute_reply.started":"2025-01-18T19:12:38.215159Z","shell.execute_reply":"2025-01-18T19:13:12.111225Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def generate_response(prompt, model):\n\n    encoded_input = tokenizer(prompt,\n                              return_tensors=\"pt\",\n                              add_special_tokens=True)\n\n    model_inputs = encoded_input.to(\"cuda\")\n\n    generated_ids = model.generate(**model_inputs,\n                                   max_new_tokens=512,\n                                   do_sample=True,\n                                   use_cache=False,\n                                   pad_token_id=tokenizer.eos_token_id)\n\n    decoded_output = tokenizer.batch_decode(generated_ids)\n\n    return decoded_output[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:13:12.112982Z","iopub.execute_input":"2025-01-18T19:13:12.113298Z","iopub.status.idle":"2025-01-18T19:13:12.117573Z","shell.execute_reply.started":"2025-01-18T19:13:12.113249Z","shell.execute_reply":"2025-01-18T19:13:12.116767Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"%%time\nprompt = \"Instruction: In your role as a medical professional, address the user's medical questions and concerns. \"\nprompt += \"I have a white tab under my tounge that is not only painful when i touch it but bleeds as well. not sure what it is, or why I got it. Can you give me any advise?\"\nprompt += \" <|end_of_turn|> \"\nprompt += \"\\n\\nAI Assistant: \"\nresponse = generate_response(prompt, fine_tuned_model)\n\nfrom pprint import pprint\npprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:32:50.675974Z","iopub.execute_input":"2025-01-18T19:32:50.676280Z","iopub.status.idle":"2025-01-18T19:32:50.679745Z","shell.execute_reply.started":"2025-01-18T19:32:50.676255Z","shell.execute_reply":"2025-01-18T19:32:50.678886Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"## \"deploy\"\n# reset kernel\n\n!pip install -q accelerate peft bitsandbytes transformers trl datasets torch\n!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:26:57.240167Z","iopub.execute_input":"2025-01-18T19:26:57.240418Z","iopub.status.idle":"2025-01-18T19:27:07.812850Z","shell.execute_reply.started":"2025-01-18T19:26:57.240392Z","shell.execute_reply":"2025-01-18T19:27:07.812023Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import accelerate\nimport peft\nimport bitsandbytes\nimport transformers\nimport trl\nimport datasets\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers import TrainingArguments\nfrom peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:27:07.813842Z","iopub.execute_input":"2025-01-18T19:27:07.814161Z","iopub.status.idle":"2025-01-18T19:27:11.626083Z","shell.execute_reply.started":"2025-01-18T19:27:07.814132Z","shell.execute_reply":"2025-01-18T19:27:11.625460Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport shutil\n# from huggingface_hub import HfApi\n# from peft import PeftModel\n\ndef load_model_with_memory_optimization(\n    base_model_name,\n    adapter_path,\n    use_4bit=True,\n    device_map=\"auto\"\n    ):\n    \"\"\"\n    Load the model with memory optimization\n    \"\"\"\n    # Clean CUDA memory\n    torch.cuda.empty_cache()\n    \n    # CUDA memory optimization config\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    \n    # 4-bits quantization\n    if use_4bit:\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n    else:\n        quantization_config = None\n    \n    try:\n        # Load model\n        print(\"Loading the base model...\")\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            quantization_config=quantization_config,\n            torch_dtype=torch.float16,\n            device_map=device_map,\n            offload_folder=\"offload\",  # Allow offload to CPU\n            offload_state_dict=True,   # Allow offload of state dict\n            low_cpu_mem_usage=True,    # Optimize the CPU memory usage\n        )\n        \n        # Load tokenizer\n        print(\"Load tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n        \n        # Load LoRA adapters\n        print(\"Applying LoRA adapters...\")\n        model = PeftModel.from_pretrained(\n            base_model,\n            adapter_path,\n            device_map=device_map,\n            is_trainable=False  # False if inference mode\n        )\n        \n        print(\"Model loaded with success!\")\n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"Error loading the model: {str(e)}\")\n        if torch.cuda.is_available():\n            print(f\"Available GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n            print(f\"Used GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:27:11.626892Z","iopub.execute_input":"2025-01-18T19:27:11.627123Z","iopub.status.idle":"2025-01-18T19:27:11.634394Z","shell.execute_reply.started":"2025-01-18T19:27:11.627091Z","shell.execute_reply":"2025-01-18T19:27:11.633524Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:27:11.636118Z","iopub.execute_input":"2025-01-18T19:27:11.636308Z","iopub.status.idle":"2025-01-18T19:27:11.794266Z","shell.execute_reply.started":"2025-01-18T19:27:11.636292Z","shell.execute_reply":"2025-01-18T19:27:11.793296Z"}},"outputs":[{"name":"stdout","text":"finetuned_model.zip  model_finetuned  state.db\ttrained_model.zip\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"repository_hf = \"berkeley-nest/Starling-LM-7B-alpha\"\n\nmodel, tokenizer = load_model_with_memory_optimization(\n    base_model_name=repository_hf,  # ex: \"mistralai/Mistral-7B-v0.1\"\n    adapter_path=\"./model_finetuned\",\n    use_4bit=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:28:05.281953Z","iopub.execute_input":"2025-01-18T19:28:05.282245Z"}},"outputs":[{"name":"stdout","text":"Loading the base model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724934f13128456f9d7cc8d80a4eee3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e5bae1c6294cd183d03f8a2ddc8a4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ba596d8c23c4da984dfce1a7511e396"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}