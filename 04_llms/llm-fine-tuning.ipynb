{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:57:55.514385Z","iopub.execute_input":"2025-01-15T21:57:55.514690Z","iopub.status.idle":"2025-01-15T21:57:55.520274Z","shell.execute_reply.started":"2025-01-15T21:57:55.514658Z","shell.execute_reply":"2025-01-15T21:57:55.519354Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q accelerate peft bitsandbytes transformers trl datasets torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:57:55.521019Z","iopub.execute_input":"2025-01-15T21:57:55.521303Z","iopub.status.idle":"2025-01-15T21:58:03.449289Z","shell.execute_reply.started":"2025-01-15T21:57:55.521282Z","shell.execute_reply":"2025-01-15T21:58:03.448236Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import accelerate\nimport peft\nimport bitsandbytes\nimport transformers\nimport trl\nimport datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:03.450354Z","iopub.execute_input":"2025-01-15T21:58:03.450683Z","iopub.status.idle":"2025-01-15T21:58:25.360597Z","shell.execute_reply.started":"2025-01-15T21:58:03.450623Z","shell.execute_reply":"2025-01-15T21:58:25.359928Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers import TrainingArguments\nfrom peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:25.362752Z","iopub.execute_input":"2025-01-15T21:58:25.363328Z","iopub.status.idle":"2025-01-15T21:58:28.421281Z","shell.execute_reply.started":"2025-01-15T21:58:25.363299Z","shell.execute_reply":"2025-01-15T21:58:28.420513Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(torch.cuda.device_count(), torch.cuda.get_device_name(0), torch.cuda.get_device_properties(0).total_memory / 1e9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:28.422606Z","iopub.execute_input":"2025-01-15T21:58:28.422845Z","iopub.status.idle":"2025-01-15T21:58:28.428255Z","shell.execute_reply.started":"2025-01-15T21:58:28.422824Z","shell.execute_reply":"2025-01-15T21:58:28.426753Z"}},"outputs":[{"name":"stdout","text":"2 Tesla T4 15.828320256\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset = load_dataset(\"nlpie/Llama2-MedTuned-Instructions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:28.429344Z","iopub.execute_input":"2025-01-15T21:58:28.429747Z","iopub.status.idle":"2025-01-15T21:58:33.655565Z","shell.execute_reply.started":"2025-01-15T21:58:28.429709Z","shell.execute_reply":"2025-01-15T21:58:33.654925Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"255c59866fc84e3093648e83c87058cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-a8790d88efc2bc45.parquet:   0%|          | 0.00/91.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4582234fedbc42baaa830d3106ea214c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-b543c64b1786c03e.parquet:   0%|          | 0.00/6.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfb964b62f646d599bb84808233facc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/200252 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db81189856874355a8bea559c082f660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/70066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8975214180424e15be2f9034cd79fabc"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:33.656374Z","iopub.execute_input":"2025-01-15T21:58:33.656676Z","iopub.status.idle":"2025-01-15T21:58:33.661506Z","shell.execute_reply.started":"2025-01-15T21:58:33.656653Z","shell.execute_reply":"2025-01-15T21:58:33.660733Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 200252\n    })\n    validation: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 70066\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Show the first 3 rows\nfor i in range(3):\n    data = dataset['train'][i]\n    print(f\"Data Point {i + 1}:\")\n    print(\"Instruction >>>\", data['instruction'])\n    print(\"Input       >>>\", data['input'])\n    print(\"Output      >>>\", data['output'])\n    print(\"\\n-----------------------------\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:33.662510Z","iopub.execute_input":"2025-01-15T21:58:33.662773Z","iopub.status.idle":"2025-01-15T21:58:33.748520Z","shell.execute_reply.started":"2025-01-15T21:58:33.662753Z","shell.execute_reply":"2025-01-15T21:58:33.747604Z"}},"outputs":[{"name":"stdout","text":"Data Point 1:\nInstruction >>> In your role as a medical professional, address the user's medical questions and concerns.\nInput       >>> My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.\nOutput      >>> Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\n\n-----------------------------\n\nData Point 2:\nInstruction >>> Your role as a doctor requires you to answer the medical questions taking into account the patient's description.\nAnalyze the question given its context. Give both long answer and yes/no decision.\nInput       >>> ###Question: Are fibrocytes involved in inflammation as well as fibrosis in the pathogenesis of Crohn 's disease?\n###Context: We previously showed that fibrocytes, a hematopoietic stem cell source of fibroblasts/myofibroblasts, infiltrated the colonic mucosa of a murine colitis model. We investigated whether fibrocytes were involved in the pathogenesis of Crohn's disease. Human surgical intestinal specimens were stained with anti-leukocyte-specific protein 1 and anti-collagen type-I (ColI) antibodies. Circulating fibrocytes in the human peripheral blood were quantified by fluorescence-activated cell sorting with anti-CD45 and anti-ColI antibodies. Cultured human fibrocytes were prepared by culturing peripheral CD14(+) monocytes. In the specimens of patients with Crohn's disease, the fibrocyte/total leukocyte percentage was significantly increased in inflammatory lesions (22.2 %, p < 0.01) compared with that in non-affected areas of the intestine (2.5 %). Interestingly, the percentage in fibrotic lesions was similar (2.2 %, p = 0.87) to that in non-affected areas. The percentages of circulating fibrocytes/total leukocytes were significantly higher in patients with Crohn's disease than in healthy controls. Both CXC-chemokine receptor 4(+) and intercellular adhesion molecule 1(+) fibrocyte numbers were significantly increased in Crohn's disease, suggesting that circulating fibrocytes have a higher ability to infiltrate injured sites and traffic leukocytes. In cultured fibrocytes, lipopolysaccharide treatment remarkably upregulated tumor necrosis factor (TNF)-α mRNA (17.0 ± 5.7-fold) and ColI mRNA expression (12.8 ± 5.7-fold), indicating that fibrocytes stimulated by bacterial components directly augmented inflammation as well as fibrosis.\nOutput      >>> Fibrocytes are recruited early in the inflammatory phase and likely differentiate into fibroblasts/myofibroblasts until the fibrosis phase. They may enhance inflammation by producing TNF-α and can directly augment fibrosis by producing ColI.\n\n###Answer: yes\n\n-----------------------------\n\nData Point 3:\nInstruction >>> Your identity is a doctor, kindly provide answers to the medical questions with consideration of the patient's description.\nAnalyze the question and answer with the best option.\nInput       >>> ###Question: Afterhyperpolarization due to\n###Options:\nA. Na efflux\nB. Na+ influx\nC. CI influx\nD. K+ efflux\n\nOutput      >>> ###Rationale: Slow return of the K+ channels to the closed state thus K+ efflux.(Ref: Textbook of physiology AK Jain 5th edition page no.36)\n\n###Answer: OPTION D IS CORRECT.\n\n-----------------------------\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# selecting some data to train the model fast\ndataset[\"train\"] = dataset[\"train\"].select(range(3500))\ndataset[\"test\"]  = dataset[\"train\"].select(range(300))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:33.749480Z","iopub.execute_input":"2025-01-15T21:58:33.749746Z","iopub.status.idle":"2025-01-15T21:58:33.764345Z","shell.execute_reply.started":"2025-01-15T21:58:33.749726Z","shell.execute_reply":"2025-01-15T21:58:33.763635Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:33.765200Z","iopub.execute_input":"2025-01-15T21:58:33.765459Z","iopub.status.idle":"2025-01-15T21:58:33.784792Z","shell.execute_reply.started":"2025-01-15T21:58:33.765439Z","shell.execute_reply":"2025-01-15T21:58:33.784079Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 3500\n    })\n    validation: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 70066\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'source'],\n        num_rows: 300\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# creating the prompt \ndef create_prompt(sample):\n    prompt = sample[\"instruction\"]\n    prompt += sample[\"input\"]\n    \n    single_turn_prompt = f\"\"\"Instruction: {prompt}<|end_of_turn|>AI Assistant: {sample[\"output\"]}\"\"\"\n    return single_turn_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:09:11.426354Z","iopub.execute_input":"2025-01-15T23:09:11.426683Z","iopub.status.idle":"2025-01-15T23:09:11.430674Z","shell.execute_reply.started":"2025-01-15T23:09:11.426637Z","shell.execute_reply":"2025-01-15T23:09:11.429752Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"create_prompt(dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:58:33.799896Z","iopub.execute_input":"2025-01-15T21:58:33.800160Z","iopub.status.idle":"2025-01-15T21:58:33.816812Z","shell.execute_reply.started":"2025-01-15T21:58:33.800130Z","shell.execute_reply":"2025-01-15T21:58:33.816134Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"\"Instruction: In your role as a medical professional, address the user's medical questions and concerns. My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.<|end_of_turn|>AI Assistant: Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\""},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"create_prompt(dataset['train'][10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T21:59:59.594009Z","iopub.execute_input":"2025-01-15T21:59:59.594401Z","iopub.status.idle":"2025-01-15T21:59:59.599848Z","shell.execute_reply.started":"2025-01-15T21:59:59.594373Z","shell.execute_reply":"2025-01-15T21:59:59.599106Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'Instruction: In the clinical text, your objective is to identify relationships between medical problems, treatments, and tests. Medical problems are tagged as @problem$, medical tests as @test$, and treatments as @treatment$. Classify the relationship between two entities as one of the following:\\nTreatment improves medical problem (TrIP)\\nTreatment worsens medical problem (TrWP)\\nTreatment causes medical problem (TrCP)\\nTreatment is administered for medical problem (TrAP)\\nTreatment is not administered because of medical problem (TrNAP)\\nTest reveals medical problem (TeRP)\\nTest conducted to investigate medical problem (TeCP)\\nMedical problem indicates medical problem (PIP)\\nNo Relations Digoxin 0.125 mg q.d. , @treatment$ 80 mg q.a.m. and 40 mg q.p.m. aspirin 1 q.d. , and @treatment$ three puffs b.i.d.<|end_of_turn|>AI Assistant: No Relations'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Quantization step\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_compute_dtype=\"float16\",\n                                bnb_4bit_use_double_quant=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:06:37.562283Z","iopub.execute_input":"2025-01-15T22:06:37.562596Z","iopub.status.idle":"2025-01-15T22:06:37.567985Z","shell.execute_reply.started":"2025-01-15T22:06:37.562575Z","shell.execute_reply":"2025-01-15T22:06:37.567114Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Loading the LLM\n# https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\n\nrepository_hf = \"berkeley-nest/Starling-LM-7B-alpha\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:09:39.930526Z","iopub.execute_input":"2025-01-15T22:09:39.930896Z","iopub.status.idle":"2025-01-15T22:09:39.934634Z","shell.execute_reply.started":"2025-01-15T22:09:39.930869Z","shell.execute_reply":"2025-01-15T22:09:39.933832Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Load the LLM applying quantization\nllm_model = AutoModelForCausalLM.from_pretrained(\n    repository_hf,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    use_cache=False\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:12:23.684867Z","iopub.execute_input":"2025-01-15T22:12:23.685177Z","iopub.status.idle":"2025-01-15T22:18:55.996554Z","shell.execute_reply.started":"2025-01-15T22:12:23.685155Z","shell.execute_reply":"2025-01-15T22:18:55.995878Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122b1296235a44adad847b5b32909758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f314ec575efc41d28a481a5990dc7aa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9756a51f509144cd85ec5edc519d5201"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20aac4c90e6241dc84c4da5ad4b5fa8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a525a311944e24a2f79890d5bd7f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d835b2b9cd5c431d8e0d33a48756bb43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0062e8f9a32a41a08e8686943bc6f662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62489b2d42d24f1a987bd108f77930ea"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Load the LLM tokenizer\ntokenizer = AutoTokenizer.from_pretrained(repository_hf)\n\n# Padding \ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:27:35.990954Z","iopub.execute_input":"2025-01-15T22:27:35.991283Z","iopub.status.idle":"2025-01-15T22:27:37.075415Z","shell.execute_reply.started":"2025-01-15T22:27:35.991258Z","shell.execute_reply":"2025-01-15T22:27:37.074683Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e68b6a7a56414e48954baac78e5601e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e5c0661e5841c1adb1e1f58dede558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f8895d86a14279bb9c3abae4b44e06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2e47a04e3c44678612084ffeffb29e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7293b654be4741558f41e38a3edf1819"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"tokenizer.eos_token, tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:30:07.615164Z","iopub.execute_input":"2025-01-15T22:30:07.615453Z","iopub.status.idle":"2025-01-15T22:30:07.620481Z","shell.execute_reply.started":"2025-01-15T22:30:07.615431Z","shell.execute_reply":"2025-01-15T22:30:07.619591Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('<|end_of_turn|>', 32000)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def generate_response_before_fine_tuning(prompt, model):\n\n    # Apply tokenizer\n    encoded_input = tokenizer(prompt,\n                              return_tensors=\"pt\", # pytorch\n                              add_special_tokens=True)\n\n    # Input to tensor\n    model_inputs = encoded_input.to(\"cuda\")\n\n    # Generate response\n    generated_ids = model.generate(**model_inputs,\n                                   max_new_tokens=1024,\n                                   do_sample=True,\n                                   pad_token_id=tokenizer.eos_token_id)\n\n    # Decoding the response\n    decoded_output = tokenizer.batch_decode(generated_ids)\n\n    return decoded_output[0].replace(prompt, \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:30:28.556347Z","iopub.execute_input":"2025-01-15T22:30:28.556632Z","iopub.status.idle":"2025-01-15T22:30:28.561078Z","shell.execute_reply.started":"2025-01-15T22:30:28.556612Z","shell.execute_reply":"2025-01-15T22:30:28.560350Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Example\nprompt = \"\"\"Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\nContradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. \"\"\"\nprompt += '''Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|>'''\nprompt += \"AI Assistant:\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:31:32.961990Z","iopub.execute_input":"2025-01-15T22:31:32.962296Z","iopub.status.idle":"2025-01-15T22:31:32.966517Z","shell.execute_reply.started":"2025-01-15T22:31:32.962275Z","shell.execute_reply":"2025-01-15T22:31:32.965628Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"generate_response_before_fine_tuning(prompt, llm_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:46:41.287100Z","iopub.execute_input":"2025-01-15T22:46:41.287415Z","iopub.status.idle":"2025-01-15T22:46:41.874559Z","shell.execute_reply.started":"2025-01-15T22:46:41.287394Z","shell.execute_reply":"2025-01-15T22:46:41.873630Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'<s> Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\\nContradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|> AI Assistant: Entailment<|end_of_turn|>'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Define LoRA parameters with PEFT\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:46:47.773133Z","iopub.execute_input":"2025-01-15T22:46:47.773459Z","iopub.status.idle":"2025-01-15T22:46:47.777122Z","shell.execute_reply.started":"2025-01-15T22:46:47.773434Z","shell.execute_reply":"2025-01-15T22:46:47.776425Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Prepare the model for the fine-tuning step\nllm_model = prepare_model_for_kbit_training(llm_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:47:46.088368Z","iopub.execute_input":"2025-01-15T22:47:46.088720Z","iopub.status.idle":"2025-01-15T22:47:46.099141Z","shell.execute_reply.started":"2025-01-15T22:47:46.088691Z","shell.execute_reply":"2025-01-15T22:47:46.098467Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Concatenate the base model with the LoRA parameters\nllm_model = get_peft_model(llm_model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:48:52.813445Z","iopub.execute_input":"2025-01-15T22:48:52.813805Z","iopub.status.idle":"2025-01-15T22:48:52.949553Z","shell.execute_reply.started":"2025-01-15T22:48:52.813777Z","shell.execute_reply":"2025-01-15T22:48:52.948662Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# # Model hyperparameters\n# training_arguments = TrainingArguments(output_dir=\"model_finetuned\",\n#                                        per_device_train_batch_size=1,\n#                                        gradient_accumulation_steps=4,\n#                                        optim=\"paged_adamw_32bit\",\n#                                        learning_rate=2e-4,\n#                                        lr_scheduler_type=\"cosine\",\n#                                        save_strategy=\"epoch\",\n#                                        logging_steps=10,\n#                                        num_train_epochs=1,\n#                                        max_steps=250,\n#                                        fp16=torch.cuda.is_available(), # True\n#                                        disable_tqdm=False\n# )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:27:16.364718Z","iopub.execute_input":"2025-01-15T23:27:16.365074Z","iopub.status.idle":"2025-01-15T23:27:16.402272Z","shell.execute_reply.started":"2025-01-15T23:27:16.365047Z","shell.execute_reply":"2025-01-15T23:27:16.401624Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import torch\n\nprint(\"Is GPU available?\", torch.cuda.is_available())\nprint(\"Using device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n\nfrom transformers import logging\n\nlogging.set_verbosity_info()  # Set verbosity to show detailed information\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./model_finetuned\",         # Output directory\n    per_device_train_batch_size=4,  # Adjust batch size to GPU memory\n    per_device_eval_batch_size=4,   # Adjust evaluation batch size\n    num_train_epochs=1,             # Number of training epochs\n    logging_dir=\"./logs\",           # Logging directory\n    evaluation_strategy=\"steps\",    # Evaluation strategy\n    save_steps=10,                  # Save every 10 steps\n    save_total_limit=2,             # Keep only last 2 models\n    logging_steps=5,                # Log every 5 steps\n    report_to=\"none\",               # Disable logging services like WandB\n    load_best_model_at_end=True,    # Automatically load the best model at the end\n    fp16=torch.cuda.is_available(), # Use mixed precision if GPU is available\n    optim=\"paged_adamw_32bit\",\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    disable_tqdm=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:28:56.896185Z","iopub.execute_input":"2025-01-15T23:28:56.896469Z","iopub.status.idle":"2025-01-15T23:28:56.933572Z","shell.execute_reply.started":"2025-01-15T23:28:56.896448Z","shell.execute_reply":"2025-01-15T23:28:56.932715Z"}},"outputs":[{"name":"stderr","text":"using `logging_steps` to initialize `eval_steps` to 5\nPyTorch: setting up devices\n","output_type":"stream"},{"name":"stdout","text":"Is GPU available? True\nUsing device: cuda\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"def create_prompt(sample):\n    prompt = sample[\"instruction\"]\n    prompt += sample[\"input\"]\n    \n    single_turn_prompt = f\"\"\"Instruction: {prompt}<|end_of_turn|>AI Assistant: {sample[\"output\"]}\"\"\"\n    return [single_turn_prompt]  # Wrap the result in a list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:28:59.056435Z","iopub.execute_input":"2025-01-15T23:28:59.056744Z","iopub.status.idle":"2025-01-15T23:28:59.060792Z","shell.execute_reply.started":"2025-01-15T23:28:59.056719Z","shell.execute_reply":"2025-01-15T23:28:59.059847Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Supervised Fine-Tuning Trainer (SFTT) https://huggingface.co/docs/trl/sft_trainer\ntrainer = SFTTrainer(     \n    model=llm_model,\n    peft_config=peft_config,\n    # max_seq_length=512,\n    tokenizer=tokenizer,\n    # packing=True,\n    formatting_func=create_prompt,\n    args=training_arguments,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"]\n                   )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:29:01.998435Z","iopub.execute_input":"2025-01-15T23:29:01.998789Z","iopub.status.idle":"2025-01-15T23:29:02.096546Z","shell.execute_reply.started":"2025-01-15T23:29:01.998762Z","shell.execute_reply":"2025-01-15T23:29:02.095719Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nYou have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\nUsing auto half precision backend\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"%%time \ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:29:06.205884Z","iopub.execute_input":"2025-01-15T23:29:06.206211Z","iopub.status.idle":"2025-01-15T23:29:19.616330Z","shell.execute_reply.started":"2025-01-15T23:29:06.206184Z","shell.execute_reply":"2025-01-15T23:29:19.613026Z"}},"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 4\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1\n  Number of trainable parameters = 3,407,872\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./model_finetuned/checkpoint-1\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--berkeley-nest--Starling-LM-7B-alpha/snapshots/1dddf3b95bc1391f6307299eb1c162c194bde9bd/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"openchat/openchat_3.5\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32002\n}\n\ntokenizer config file saved in ./model_finetuned/checkpoint-1/tokenizer_config.json\nSpecial tokens file saved in ./model_finetuned/checkpoint-1/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 7.32 s, sys: 6 s, total: 13.3 s\nWall time: 13.4 s\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1, training_loss=1.2472082376480103, metrics={'train_runtime': 13.024, 'train_samples_per_second': 0.307, 'train_steps_per_second': 0.077, 'total_flos': 174835535708160.0, 'train_loss': 1.2472082376480103, 'epoch': 1.0})"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"trainer.save_model(\"model_finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:29:19.860853Z","iopub.execute_input":"2025-01-15T23:29:19.861080Z","iopub.status.idle":"2025-01-15T23:29:20.068720Z","shell.execute_reply.started":"2025-01-15T23:29:19.861057Z","shell.execute_reply":"2025-01-15T23:29:20.067847Z"}},"outputs":[{"name":"stderr","text":"Saving model checkpoint to model_finetuned\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--berkeley-nest--Starling-LM-7B-alpha/snapshots/1dddf3b95bc1391f6307299eb1c162c194bde9bd/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"openchat/openchat_3.5\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32002\n}\n\ntokenizer config file saved in model_finetuned/tokenizer_config.json\nSpecial tokens file saved in model_finetuned/special_tokens_map.json\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"fine_tuned_model = llm_model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:29:46.041902Z","iopub.execute_input":"2025-01-15T23:29:46.042241Z","iopub.status.idle":"2025-01-15T23:29:49.414592Z","shell.execute_reply.started":"2025-01-15T23:29:46.042214Z","shell.execute_reply":"2025-01-15T23:29:49.413740Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def generate_response(prompt, model):\n\n    encoded_input = tokenizer(prompt,\n                              return_tensors=\"pt\",\n                              add_special_tokens=True)\n\n    model_inputs = encoded_input.to(\"cuda\")\n\n    generated_ids = model.generate(**model_inputs,\n                                   max_new_tokens=512,\n                                   do_sample=True,\n                                   use_cache=False,\n                                   pad_token_id=tokenizer.eos_token_id)\n\n    decoded_output = tokenizer.batch_decode(generated_ids)\n\n    return decoded_output[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:29:52.763338Z","iopub.execute_input":"2025-01-15T23:29:52.763634Z","iopub.status.idle":"2025-01-15T23:29:52.768118Z","shell.execute_reply.started":"2025-01-15T23:29:52.763613Z","shell.execute_reply":"2025-01-15T23:29:52.767214Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"%%time\nprompt = \"Instruction: In your role as a medical professional, address the user's medical questions and concerns. \"\nprompt += \"I have a white tab under my tounge that is not only painful when i touch it but bleeds as well. not sure what it is, or why I got it. Can you give me any advise? <|end_of_turn|> \"\nprompt += \"AI Assistant:\"\nresponse = generate_response(prompt, fine_tuned_model)\n\nfrom pprint import pprint\npprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T23:30:38.599776Z","iopub.execute_input":"2025-01-15T23:30:38.600095Z","iopub.status.idle":"2025-01-15T23:32:20.026071Z","shell.execute_reply.started":"2025-01-15T23:30:38.600071Z","shell.execute_reply":"2025-01-15T23:32:20.025194Z"}},"outputs":[{"name":"stdout","text":"(\"<s> Instruction: In your role as a medical professional, address the user's \"\n 'medical questions and concerns. I have a white tab under my tounge that is '\n 'not only painful when i touch it but bleeds as well. not sure what it is, or '\n \"why I got it. Can you give me any advise? <|end_of_turn|>  AI Assistant: I'm \"\n \"not a doctor, but I can offer some general advice. It's important to get any \"\n 'unusual oral symptoms, like a painful and bleeding spot under your tongue, '\n 'checked out by a healthcare professional. It could potentially be due to a '\n 'variety of causes, including infection, injury, or oral disease. \\n'\n '\\n'\n \"As with any medical concerns, it's important to get a proper diagnosis from \"\n 'a qualified healthcare provider. They can perform an evaluation and '\n 'recommend treatment options if necessary.\\n'\n '\\n'\n 'Here are some additional tips that might help:\\n'\n '\\n'\n '1. Maintain good oral hygiene by brushing your teeth at least twice a day '\n 'and flossing regularly to remove food particles and plaque from between your '\n 'teeth and around your gum line.\\n'\n '2. Drink plenty of water to stay hydrated, as dehydration can sometimes lead '\n 'to a drier mouth and increased risk of oral issues.\\n'\n '3. Avoid smoking and limit or avoid alcohol consumption, as both can '\n 'contribute to oral health problems.\\n'\n '4. Eat a balanced diet, incorporating fruits, vegetables, and whole grains '\n 'to promote overall health.\\n'\n '\\n'\n 'Always check with a healthcare professional for advice on addressing '\n 'specific health concerns, including those related to your oral '\n 'health.<|end_of_turn|>')\nCPU times: user 1min 25s, sys: 16.2 s, total: 1min 41s\nWall time: 1min 41s\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}