{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste rapido de calculo de embeddings com chatGPT\n",
    "corpus = [\n",
    "    \"o gato comeu o rato\",\n",
    "    \"o cachorro correu atrás do gato\",\n",
    "    \"o pássaro voou para longe\",\n",
    "    \"o peixe nadou no rio\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['o', 'gato', 'comeu', 'o', 'rato'],\n",
       " ['o', 'cachorro', 'correu', 'atrás', 'do', 'gato'],\n",
       " ['o', 'pássaro', 'voou', 'para', 'longe'],\n",
       " ['o', 'peixe', 'nadou', 'no', 'rio']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização simples\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'atrás': 9,\n",
      " 'cachorro': 11,\n",
      " 'comeu': 0,\n",
      " 'correu': 1,\n",
      " 'do': 12,\n",
      " 'gato': 14,\n",
      " 'longe': 10,\n",
      " 'nadou': 7,\n",
      " 'no': 3,\n",
      " 'o': 8,\n",
      " 'para': 2,\n",
      " 'peixe': 5,\n",
      " 'pássaro': 6,\n",
      " 'rato': 13,\n",
      " 'rio': 4,\n",
      " 'voou': 15}\n"
     ]
    }
   ],
   "source": [
    "# Criar vocabulário\n",
    "vocab = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "vocab = list(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Mapeamento palavra <-> índice\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Coocorrência:\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 1., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [2., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 2., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 2., 1., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Inicializar matriz de coocorrência\n",
    "cooccurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32) # Da pra interpretar como um grafo\n",
    "\n",
    "# Janela de contexto\n",
    "window_size = 2\n",
    "\n",
    "# Preencher matriz de coocorrência\n",
    "for sentence in tokenized_corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        word_idx = word_to_index[word]\n",
    "        # Determinar janela de contexto\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, len(sentence))\n",
    "        # Adicionar coocorrências\n",
    "        for j in range(start, end):\n",
    "            if i != j:  # Ignorar a palavra atual\n",
    "                context_idx = word_to_index[sentence[j]]\n",
    "                cooccurrence_matrix[word_idx, context_idx] += 1\n",
    "\n",
    "print(\"Matriz de Coocorrência:\")\n",
    "pprint(cooccurrence_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Calculados:\n",
      "'comeu: [ 1.7673157e+00 -8.6055112e-01 -1.9129515e-02  3.4633558e-08]'\n",
      "'correu: [ 1.1843683e+00 -8.3020598e-01  8.3977550e-01 -7.7918958e-07]'\n",
      "'para: [ 0.4168095   0.32144898 -0.76711947  1.0095162 ]'\n",
      "'no: [ 0.4168095   0.32144862 -0.7671209  -1.0095146 ]'\n",
      "'rio: [ 0.27068844  0.08879717 -0.5862039  -0.78820515]'\n",
      "'peixe: [ 0.8603785  -0.6085834  -0.67432    -0.78820497]'\n",
      "'pássaro: [ 0.8603785  -0.608583   -0.67431855  0.7882065 ]'\n",
      "'nadou: [ 0.9063503  -0.6425294  -0.83159584 -1.0095142 ]'\n",
      "'o: [ 2.8824861e+00  2.5216212e+00 -2.4031439e-01 -2.0861626e-07]'\n",
      "'atrás: [ 1.0213596e+00  6.9658637e-01  1.0447264e+00 -1.2479941e-06]'\n",
      "'longe: [ 0.27068844  0.08879752 -0.58620244  0.78820634]'\n",
      "'cachorro: [ 1.0409317e+00 -6.6042697e-01  6.0287690e-01 -4.4111221e-07]'\n",
      "'do: [ 8.4457457e-01  4.4411576e-01  8.8297749e-01 -8.7934313e-07]'\n",
      "'rato: [ 9.5124280e-01 -4.5938718e-01 -9.5130488e-02  6.4552296e-08]'\n",
      "'gato: [ 1.9226600e+00 -1.4722406e+00  5.2358735e-01 -4.3179170e-07]'\n",
      "'voou: [ 0.9063503 -0.6425291 -0.8315943  1.0095167]'\n"
     ]
    }
   ],
   "source": [
    "# Reduzir dimensão para embeddings\n",
    "embedding_dim = 4\n",
    "\n",
    "svd = TruncatedSVD(n_components=embedding_dim)\n",
    "word_embeddings = svd.fit_transform(cooccurrence_matrix)\n",
    "\n",
    "print(\"Embeddings Calculados:\")\n",
    "for word, embedding in zip(vocab, word_embeddings):\n",
    "    pprint(f\"{word}: {embedding}\") # Lembre que cada linha do embedding representa uma palavra\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
